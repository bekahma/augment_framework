{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to analyze the automatic detection tools against human annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import ast\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/c/clea.chataigner/multi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "modification='prepositions' #change here to the modification you want to analyze\n",
    "models=['chatgpt', \"deepseek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "DATA_FOLDER='../data/paraphrases/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of settings without paraphrases for chatgpt: 0\n",
      "Number of settings without paraphrases for deepseek: 3\n"
     ]
    }
   ],
   "source": [
    "all_dfs=[]\n",
    "for model in models:\n",
    "    ANNOTATED_FILE=DATA_FOLDER+f\"Gender_identity_{modification}_{model}_annotated.xlsx\"\n",
    "    model_df=pd.read_excel(ANNOTATED_FILE)\n",
    "    model_df['model'] = model  # Add model column\n",
    "    model_df['unique_id'] = model_df['idx'].astype(str) + '_' + model_df['disambiguated'].astype(str)\n",
    "    print(f\"Number of settings without paraphrases for {model}:\", 120-len(model_df.unique_id.unique()))\n",
    "    all_dfs.append(model_df)\n",
    "all_annotated_df=pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "if modification=='prepositions':\n",
    "    all_annotated_df[\"wrong_added\"]=all_annotated_df[\"wrong_added\"].apply(ast.literal_eval)\n",
    "    all_annotated_df[\"wrong_removed\"]=all_annotated_df[\"wrong_removed\"].apply(ast.literal_eval)\n",
    "elif modification=='AAE' or modification==\"formal\":\n",
    "    all_annotated_df[\"proba_par\"]=all_annotated_df[\"proba_par\"].apply(lambda x : round(x,2))\n",
    "    all_annotated_df[\"proba_ori\"]=all_annotated_df[\"proba_ori\"].apply(lambda x : round(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human annotation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Evaluation metrics by model}\n",
      "\\label{tab:model_results}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & chatgpt & deepseek \\\\\n",
      "\\midrule\n",
      "Keep Rate (per Input) (\\%) & 85.7 & 82.5 \\\\\n",
      "Overall Keep Rate (%) & 84.9 & 65.2 \\\\\n",
      "Valid Paraphrases per Input (\\%) & 84.3 & 64.7 \\\\\n",
      "Paraphrases per Input & 1.2 & 3.3 \\\\\n",
      "Unmodified Sentences (\\%) & 0.7 & 0.8 \\\\\n",
      "Edits per Paraphrase & 4.0 & 3.6 \\\\\n",
      "Incorrect Modifications (%) & 27.3 & 79.2 \\\\\n",
      "Realism Errors (%) & 72.7 & 20.0 \\\\\n",
      "Meaning Errors (%) & 0.0 & 2.3 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To do the table 2 in the paper\n",
    "# Dict to collect stats per model\n",
    "model_stats = {}\n",
    "for model in models:\n",
    "    annotated_df = all_annotated_df[all_annotated_df.model == model].copy()\n",
    "    no_modif_count = (annotated_df.nb_modif == 0).sum()\n",
    "    annotated_df = annotated_df[(annotated_df.nb_modif != 0) & (annotated_df.keep.notna())]\n",
    "    annotated_df['keep']=annotated_df['keep'].astype(bool)\n",
    "    df_grouped = annotated_df.groupby('unique_id', as_index=False).agg(\n",
    "        keep_any=('keep', 'any'),\n",
    "        keep_sum=('keep', 'sum'),\n",
    "        keep_total=('keep', 'count')\n",
    "    )\n",
    "    df_grouped['keep_pct'] = df_grouped['keep_sum'] / df_grouped['keep_total']\n",
    "    n_errors=len(annotated_df[~annotated_df.keep])\n",
    "    model_stats[model] = {\n",
    "        \"Keep Rate (per Input) (\\%)\": round(df_grouped['keep_any'].sum() / len(df_grouped) *100, 1),\n",
    "        \"Overall Keep Rate (%)\": round(annotated_df['keep'].mean() *100, 1),\n",
    "        \"Valid Paraphrases per Input (\\%)\": round(df_grouped['keep_pct'].mean() *100, 1),\n",
    "        \"Paraphrases per Input\": round(df_grouped['keep_total'].mean(), 2),\n",
    "        \"Unmodified Sentences (\\%)\": round(no_modif_count/len(annotated_df)*100, 1),\n",
    "        \"Edits per Paraphrase\": round(annotated_df['nb_modif'].mean(), 2),\n",
    "        \"Incorrect Modifications (%)\": round(annotated_df['wrong_modif'].notna().sum()/n_errors *100, 1),\n",
    "        \"Realism Errors (%)\": round(annotated_df['realism'].notna().sum()/n_errors *100, 1),\n",
    "        \"Meaning Errors (%)\": round(annotated_df['meaning'].notna().sum()/n_errors *100, 1),\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame and transpose\n",
    "df_latex = pd.DataFrame(model_stats).T.transpose()\n",
    "\n",
    "# Create LaTeX table\n",
    "latex_table = df_latex.to_latex(float_format=\"%.1f\", index=True, caption=\"Evaluation metrics by model\", label=\"tab:model_results\")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out sentences with no modification and no annotation\n",
    "all_annotated_df=all_annotated_df[all_annotated_df.nb_modif!=0]\n",
    "all_annotated_df=all_annotated_df[all_annotated_df.keep.notna()]\n",
    "all_annotated_df['keep']=all_annotated_df['keep'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    520.000000\n",
      "mean       0.939282\n",
      "std        0.032248\n",
      "min        0.700000\n",
      "25%        0.918919\n",
      "50%        0.947368\n",
      "75%        0.963799\n",
      "max        0.984848\n",
      "Name: rouge_l, dtype: float64\n",
      "count    520.000000\n",
      "mean       0.993460\n",
      "std        0.005047\n",
      "min        0.957781\n",
      "25%        0.991688\n",
      "50%        0.994569\n",
      "75%        0.996738\n",
      "max        0.999955\n",
      "Name: bert_score, dtype: float64\n",
      "count    520.000000\n",
      "mean       0.982204\n",
      "std        0.019656\n",
      "min        0.879202\n",
      "25%        0.985612\n",
      "50%        0.988706\n",
      "75%        0.990024\n",
      "max        0.991226\n",
      "Name: sbert_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Similarity metrics analysis\n",
    "print(all_annotated_df.rouge_l.describe())\n",
    "print(all_annotated_df.bert_score.describe())\n",
    "print(all_annotated_df.sbert_score.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    520.000000\n",
      "mean       1.215616\n",
      "std        0.254187\n",
      "min        0.653160\n",
      "25%        1.041365\n",
      "50%        1.146089\n",
      "75%        1.329385\n",
      "max        2.317677\n",
      "Name: perplexity_ratio, dtype: float64\n",
      "count    520.000000\n",
      "mean      28.567284\n",
      "std       19.265088\n",
      "min        9.023422\n",
      "25%       16.195745\n",
      "50%       21.096656\n",
      "75%       34.321524\n",
      "max      133.445801\n",
      "Name: perplexity_par, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Perplexity metric analysis\n",
    "all_annotated_df[\"perplexity_ratio\"]=all_annotated_df[\"perplexity_par\"]/all_annotated_df[\"perplexity_original\"]\n",
    "print(all_annotated_df.perplexity_ratio.describe())\n",
    "print(all_annotated_df.perplexity_par.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.55598074]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for thresholds\n",
    "q=all_annotated_df[[\"perplexity_ratio\"]].quantile(0.9).values\n",
    "print(q)\n",
    "len(all_annotated_df[(all_annotated_df[\"perplexity_ratio\"]<1.85)&(~all_annotated_df[\"keep\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils functions for automatic check\n",
    "def lemmatize_list(words):\n",
    "    return [nlp(w)[0].lemma_ for w in words]\n",
    "\n",
    "def compare_lemmas(row):\n",
    "    '''For prepositions modification, check if the lemmas of wrong added and wrong removed are identical'''\n",
    "    return lemmatize_list(row['wrong_added']) == lemmatize_list(row['wrong_removed'])\n",
    "\n",
    "def stem_list(words):\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "def compare_stems(row):\n",
    "    '''For prepositions modification, check if the lemmas of wrong added and wrong removed are identical'''\n",
    "    return stem_list(row['wrong_added']) == stem_list(row['wrong_removed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic rules per type of modification\n",
    "if modification=='prepositions':\n",
    "    all_annotated_df['automated_keep']=((((all_annotated_df[\"wrong_added\"].apply(lambda x: x == []))&(all_annotated_df[\"wrong_removed\"].apply(lambda x: x == [])))\n",
    "                                    |all_annotated_df.apply(compare_lemmas, axis=1)\n",
    "                                    |all_annotated_df.apply(compare_stems, axis=1))\n",
    "                                    &(all_annotated_df.perplexity_ratio<1.85)\n",
    "                                    &(all_annotated_df.sbert_score>0.8))\n",
    "\n",
    "elif modification=='AAE':\n",
    "    all_annotated_df['automated_keep']=(((all_annotated_df[\"label_par\"]=='LABEL_1')|((all_annotated_df[\"proba_par\"]<all_annotated_df[\"proba_ori\"])&(all_annotated_df[\"proba_par\"]<=0.9)))\n",
    "                                       &(all_annotated_df.sbert_score>0.75))\n",
    "elif modification=='formal':\n",
    "    all_annotated_df['automated_keep']=(((all_annotated_df[\"label_par\"]=='formal')|((all_annotated_df[\"proba_par\"]<all_annotated_df[\"proba_ori\"])))\n",
    "                                       &(all_annotated_df.perplexity_ratio<2)\n",
    "                                       &(all_annotated_df.sbert_score>0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated  False  True \n",
      "Human                  \n",
      "False        115     37\n",
      "True          40    328\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "conf_matrix = pd.crosstab(all_annotated_df['keep'], all_annotated_df['automated_keep'], rownames=['Human'], colnames=['Automated'])\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.899\n",
      "Recall:    0.891\n",
      "F1 Score:  0.895\n",
      "Accuracy:  0.852\n"
     ]
    }
   ],
   "source": [
    "# Performance Metrics\n",
    "y_true = all_annotated_df['keep']\n",
    "y_pred = all_annotated_df['automated_keep']\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>model</th>\n",
       "      <th>original</th>\n",
       "      <th>raw_answer</th>\n",
       "      <th>nb_modif</th>\n",
       "      <th>wrong_modif</th>\n",
       "      <th>realism</th>\n",
       "      <th>meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>14_False</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} were talking about...</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} were talking conce...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>48_True</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>The college counselor was giving advice to {{N...</td>\n",
       "      <td>The college counselor was giving advice to {{N...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>49_False</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} often go to the gym to...</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} often go towards the g...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>55_True</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>There's two new students in math class, {{NAME...</td>\n",
       "      <td>There's two new students during math class, {{...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>49_True</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} often go to the gym to...</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} often go towards the g...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id     model                                           original  \\\n",
       "33   14_False   chatgpt  A {{NAME1}} and a {{NAME2}} were talking about...   \n",
       "118   48_True   chatgpt  The college counselor was giving advice to {{N...   \n",
       "458  49_False  deepseek  {{NAME1}} and {{NAME2}} often go to the gym to...   \n",
       "494   55_True  deepseek  There's two new students in math class, {{NAME...   \n",
       "120   49_True   chatgpt  {{NAME1}} and {{NAME2}} often go to the gym to...   \n",
       "\n",
       "                                            raw_answer  nb_modif  wrong_modif  \\\n",
       "33   A {{NAME1}} and a {{NAME2}} were talking conce...         2          NaN   \n",
       "118  The college counselor was giving advice to {{N...         6          NaN   \n",
       "458  {{NAME1}} and {{NAME2}} often go towards the g...         2          NaN   \n",
       "494  There's two new students during math class, {{...         2          NaN   \n",
       "120  {{NAME1}} and {{NAME2}} often go towards the g...         2          NaN   \n",
       "\n",
       "     realism  meaning  \n",
       "33       1.0      NaN  \n",
       "118      1.0      NaN  \n",
       "458      1.0      NaN  \n",
       "494      1.0      NaN  \n",
       "120      1.0      NaN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#False positives examples\n",
    "all_annotated_df.loc[(~all_annotated_df[\"keep\"])&all_annotated_df[\"automated_keep\"], [\"unique_id\", \"model\",\"original\", \"raw_answer\", \"nb_modif\", \"wrong_modif\", \"realism\", \"meaning\"]].sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>original</th>\n",
       "      <th>raw_answer</th>\n",
       "      <th>wrong_added</th>\n",
       "      <th>wrong_removed</th>\n",
       "      <th>sbert_score</th>\n",
       "      <th>perplexity_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>I'm supervising two interns, {{NAME1}} and {{N...</td>\n",
       "      <td>I'm supervising two interns, {{NAME1}} and {{N...</td>\n",
       "      <td>[contrast]</td>\n",
       "      <td>[other, hand]</td>\n",
       "      <td>0.969329</td>\n",
       "      <td>1.145996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>The college counselor was giving advice to a {...</td>\n",
       "      <td>The college counselor was giving advice to a {...</td>\n",
       "      <td>[contrast]</td>\n",
       "      <td>[other, hand]</td>\n",
       "      <td>0.986378</td>\n",
       "      <td>1.129735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} have been dating f...</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} have been dating t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.924030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>The college counselor was giving advice to {{N...</td>\n",
       "      <td>The college counselor was giving advice concer...</td>\n",
       "      <td>[concerning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.990873</td>\n",
       "      <td>1.047683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} are interviewing an en...</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} are interviewing an en...</td>\n",
       "      <td>[behalf]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.977124</td>\n",
       "      <td>0.855791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model                                           original  \\\n",
       "470  deepseek  I'm supervising two interns, {{NAME1}} and {{N...   \n",
       "328  deepseek  The college counselor was giving advice to a {...   \n",
       "333  deepseek  A {{NAME1}} and a {{NAME2}} have been dating f...   \n",
       "455  deepseek  The college counselor was giving advice to {{N...   \n",
       "400  deepseek  {{NAME1}} and {{NAME2}} are interviewing an en...   \n",
       "\n",
       "                                            raw_answer   wrong_added  \\\n",
       "470  I'm supervising two interns, {{NAME1}} and {{N...    [contrast]   \n",
       "328  The college counselor was giving advice to a {...    [contrast]   \n",
       "333  A {{NAME1}} and a {{NAME2}} have been dating t...            []   \n",
       "455  The college counselor was giving advice concer...  [concerning]   \n",
       "400  {{NAME1}} and {{NAME2}} are interviewing an en...      [behalf]   \n",
       "\n",
       "     wrong_removed  sbert_score  perplexity_ratio  \n",
       "470  [other, hand]     0.969329          1.145996  \n",
       "328  [other, hand]     0.986378          1.129735  \n",
       "333             []     0.988700          1.924030  \n",
       "455             []     0.990873          1.047683  \n",
       "400             []     0.977124          0.855791  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#False negatives examples\n",
    "if modification=='prepositions':\n",
    "    display(all_annotated_df.loc[(all_annotated_df[\"keep\"])&(~all_annotated_df[\"automated_keep\"]), [\"model\", \"original\", \"raw_answer\", \"wrong_added\", \"wrong_removed\", \"sbert_score\", \"perplexity_ratio\"]].sample(n=5, random_state=42))\n",
    "elif modification=='formal' or modification=='AAE':\n",
    "    display(all_annotated_df.loc[(all_annotated_df[\"keep\"])&(~all_annotated_df[\"automated_keep\"]), [\"model\", \"original\", \"raw_answer\", \"proba_par\", \"label_par\", \"proba_ori\", \"label_ori\", \"sbert_score\", \"perplexity_ratio\"]].sample(n=5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Evaluation metrics by model}\n",
      "\\label{tab:model_results}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & chatgpt & deepseek \\\\\n",
      "\\midrule\n",
      "Perf@Any & 0.840 & 0.807 \\\\\n",
      "Valid% & 0.834 & 0.650 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see the impact of automatic filtering on the number of paraphrases per example\n",
    "#Dict to collect stats per model\n",
    "model_stats = {}\n",
    "for model in models:\n",
    "    annotated_df = all_annotated_df[all_annotated_df.model == model].copy()\n",
    "    annotated_df = annotated_df[(annotated_df.nb_modif != 0) & (annotated_df.keep.notna())]\n",
    "    df_grouped = annotated_df.groupby('unique_id', as_index=False).agg(\n",
    "        keep_any=('automated_keep', 'any'),\n",
    "        keep_sum=('automated_keep', 'sum'),\n",
    "        keep_total=('automated_keep', 'count')\n",
    "    )\n",
    "    df_grouped['keep_pct'] = df_grouped['keep_sum'] / df_grouped['keep_total']\n",
    "    model_stats[model] = {\n",
    "        \"Perf@Any\": round(df_grouped['keep_any'].sum() / len(df_grouped), 3),\n",
    "        \"Valid%\": round(df_grouped['keep_pct'].mean(), 3),\n",
    "    }\n",
    "    \n",
    "# Convert to DataFrame and transpose\n",
    "df_latex = pd.DataFrame(model_stats).T.transpose()\n",
    "\n",
    "# Create LaTeX table\n",
    "latex_table = df_latex.to_latex(float_format=\"%.3f\", index=True, caption=\"Evaluation metrics by model\", label=\"tab:model_results\")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (multi)",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
