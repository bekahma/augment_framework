{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to analyze the automatic detection tools against human annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import ast\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "modification='formal' #change here to the modification you want to analyze\n",
    "models=['chatgpt', \"deepseek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "DATA_FOLDER=f'../data/paraphrases/{modification}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of settings without paraphrases for chatgpt: 0\n",
      "Number of settings without paraphrases for deepseek: 0\n"
     ]
    }
   ],
   "source": [
    "all_dfs=[]\n",
    "for model in models:\n",
    "    ANNOTATED_FILE=DATA_FOLDER+f\"Gender_identity_{modification}_{model}_annotated.xlsx\"\n",
    "    model_df=pd.read_excel(ANNOTATED_FILE)\n",
    "    model_df['model'] = model  # Add model column\n",
    "    model_df['unique_id'] = model_df['idx'].astype(str) + '_' + model_df['disambiguated'].astype(str)\n",
    "    print(f\"Number of settings without paraphrases for {model}:\", 120-len(model_df.unique_id.unique()))\n",
    "    all_dfs.append(model_df)\n",
    "all_annotated_df=pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "if modification=='prepositions':\n",
    "    all_annotated_df[\"wrong_added\"]=all_annotated_df[\"wrong_added\"].apply(ast.literal_eval)\n",
    "    all_annotated_df[\"wrong_removed\"]=all_annotated_df[\"wrong_removed\"].apply(ast.literal_eval)\n",
    "elif modification=='AAE' or modification==\"formal\":\n",
    "    all_annotated_df[\"proba_par\"]=all_annotated_df[\"proba_par\"].apply(lambda x : round(x,2))\n",
    "    all_annotated_df[\"proba_ori\"]=all_annotated_df[\"proba_ori\"].apply(lambda x : round(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human annotation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Evaluation metrics by model for modification formal}\n",
      "\\label{tab:model_results}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & chatgpt & deepseek \\\\\n",
      "\\midrule\n",
      "Generated Paraphrases per Input & 4.5 & 4.7 \\\\\n",
      "Mean Number of Edits per Word & 0.6 & 0.7 \\\\\n",
      "Inputs Without Edits (\\%) & 0.0 & 0.5 \\\\\n",
      "Inputs with At Least One Kept Paraphrase (\\%) & 100.0 & 98.3 \\\\\n",
      "Global Proportion of Valid Paraphrases (\\%)  & 91.9 & 88.7 \\\\\n",
      "Average Valid Paraphrase Ratio per Input (\\%) & 92.7 & 88.0 \\\\\n",
      "Correctness Errors (\\%) & 84.1 & 49.2 \\\\\n",
      "Realism Errors (\\%) & 9.1 & 7.9 \\\\\n",
      "Meaning Errors (\\%) & 11.4 & 34.9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To do the table 2 in the paper\n",
    "# Dict to collect stats per model\n",
    "model_stats = {}\n",
    "for model in models:\n",
    "    annotated_df = all_annotated_df[all_annotated_df.model == model].copy()\n",
    "    no_modif_count = (annotated_df.nb_modif == 0).sum()\n",
    "    annotated_df = annotated_df[(annotated_df.nb_modif != 0) & (annotated_df.keep.notna())]\n",
    "    annotated_df['keep']=annotated_df['keep'].astype(bool)\n",
    "    annotated_df['input_len'] = annotated_df['original'].str.split().str.len()\n",
    "    annotated_df['edits_per_word'] = annotated_df['nb_modif'] / annotated_df['input_len']\n",
    "    \n",
    "    df_grouped = annotated_df.groupby('unique_id', as_index=False).agg(\n",
    "        keep_any=('keep', 'any'),\n",
    "        keep_sum=('keep', 'sum'),\n",
    "        keep_total=('keep', 'count')\n",
    "    )\n",
    "    df_grouped['keep_pct'] = df_grouped['keep_sum'] / df_grouped['keep_total']\n",
    "    n_errors=len(annotated_df[~annotated_df.keep])\n",
    "    model_stats[model] = {\n",
    "        \"Generated Paraphrases per Input\": round(df_grouped['keep_total'].mean(), 2),\n",
    "        \"Mean Number of Edits per Word\": round(annotated_df['edits_per_word'].mean(), 2),\n",
    "        \"Inputs Without Edits (\\%)\": round(no_modif_count/len(annotated_df)*100, 1),\n",
    "        \n",
    "        \"Inputs with At Least One Kept Paraphrase (\\%)\": round(df_grouped['keep_any'].sum() / len(df_grouped) *100, 1),\n",
    "        \"Global Proportion of Valid Paraphrases (\\%) \": round(annotated_df['keep'].mean() *100, 1),\n",
    "        \"Average Valid Paraphrase Ratio per Input (\\%)\": round(df_grouped['keep_pct'].mean() *100, 1),\n",
    "        \n",
    "        \"Correctness Errors (\\%)\": round(annotated_df['wrong_modif'].notna().sum()/n_errors *100, 1),\n",
    "        \"Realism Errors (\\%)\": round(annotated_df['realism'].notna().sum()/n_errors *100, 1),\n",
    "        \"Meaning Errors (\\%)\": round(annotated_df['meaning'].notna().sum()/n_errors *100, 1),\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame and transpose\n",
    "df_latex = pd.DataFrame(model_stats).T.transpose()\n",
    "\n",
    "# Create LaTeX table\n",
    "latex_table = df_latex.to_latex(float_format=\"%.1f\", index=True, caption=f\"Evaluation metrics by model for modification {modification}\", label=\"tab:model_results\")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out sentences with no modification and no annotation\n",
    "all_annotated_df=all_annotated_df[all_annotated_df.nb_modif!=0]\n",
    "all_annotated_df=all_annotated_df[all_annotated_df.keep.notna()]\n",
    "all_annotated_df['keep']=all_annotated_df['keep'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1102.000000\n",
      "mean        0.691907\n",
      "std         0.131939\n",
      "min         0.342857\n",
      "25%         0.596834\n",
      "50%         0.701754\n",
      "75%         0.787879\n",
      "max         1.000000\n",
      "Name: rouge_l, dtype: float64\n",
      "count    1102.000000\n",
      "mean        0.971726\n",
      "std         0.013439\n",
      "min         0.929204\n",
      "25%         0.963672\n",
      "50%         0.973598\n",
      "75%         0.981413\n",
      "max         0.998459\n",
      "Name: bert_score, dtype: float64\n",
      "count    1102.000000\n",
      "mean        0.931833\n",
      "std         0.070703\n",
      "min         0.563168\n",
      "25%         0.902019\n",
      "50%         0.960688\n",
      "75%         0.984177\n",
      "max         0.991305\n",
      "Name: sbert_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Similarity metrics analysis\n",
    "print(all_annotated_df.rouge_l.describe())\n",
    "print(all_annotated_df.bert_score.describe())\n",
    "print(all_annotated_df.sbert_score.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1102.000000\n",
      "mean        1.248203\n",
      "std         0.368289\n",
      "min         0.323196\n",
      "25%         0.983341\n",
      "50%         1.190847\n",
      "75%         1.449893\n",
      "max         3.037339\n",
      "Name: perplexity_ratio, dtype: float64\n",
      "count    1102.000000\n",
      "mean       27.153876\n",
      "std        13.799774\n",
      "min         8.270179\n",
      "25%        17.032090\n",
      "50%        23.053087\n",
      "75%        34.754127\n",
      "max        93.105911\n",
      "Name: perplexity_par, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Perplexity metric analysis\n",
    "all_annotated_df[\"perplexity_ratio\"]=all_annotated_df[\"perplexity_par\"]/all_annotated_df[\"perplexity_original\"]\n",
    "print(all_annotated_df.perplexity_ratio.describe())\n",
    "print(all_annotated_df.perplexity_par.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.96477269]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for thresholds\n",
    "q=all_annotated_df[[\"perplexity_ratio\"]].quantile(0.95).values\n",
    "print(q)\n",
    "len(all_annotated_df[(all_annotated_df[\"perplexity_ratio\"]>1.85)&(all_annotated_df[\"keep\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77499473]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=all_annotated_df[[\"sbert_score\"]].quantile(0.05).values\n",
    "print(q)\n",
    "len(all_annotated_df[(all_annotated_df[\"sbert_score\"]<0.8)&(~all_annotated_df[\"keep\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils functions for automatic check\n",
    "def lemmatize_list(words):\n",
    "    return [nlp(w)[0].lemma_ for w in words]\n",
    "\n",
    "def compare_lemmas(row):\n",
    "    '''For prepositions modification, check if the lemmas of wrong added and wrong removed are identical'''\n",
    "    return lemmatize_list(row['wrong_added']) == lemmatize_list(row['wrong_removed'])\n",
    "\n",
    "def stem_list(words):\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "def compare_stems(row):\n",
    "    '''For prepositions modification, check if the lemmas of wrong added and wrong removed are identical'''\n",
    "    return stem_list(row['wrong_added']) == stem_list(row['wrong_removed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic rules per type of modification\n",
    "if modification=='prepositions':\n",
    "    all_annotated_df['automated_keep']=((((all_annotated_df[\"wrong_added\"].apply(lambda x: x == []))&(all_annotated_df[\"wrong_removed\"].apply(lambda x: x == [])))\n",
    "                                    |all_annotated_df.apply(compare_lemmas, axis=1)\n",
    "                                    |all_annotated_df.apply(compare_stems, axis=1))\n",
    "                                    &(all_annotated_df.perplexity_ratio<1.85)\n",
    "                                    &(all_annotated_df.sbert_score>0.8))\n",
    "\n",
    "elif modification=='AAE':\n",
    "    all_annotated_df['automated_keep']=(((all_annotated_df[\"label_par\"]=='LABEL_1')|((all_annotated_df[\"proba_par\"]<all_annotated_df[\"proba_ori\"])&(all_annotated_df[\"proba_par\"]<=0.9)))\n",
    "                                       &(all_annotated_df.sbert_score>0.75))\n",
    "elif modification=='formal':\n",
    "    all_annotated_df['automated_keep']=(((all_annotated_df[\"label_par\"]=='formal')|((all_annotated_df[\"proba_par\"]<all_annotated_df[\"proba_ori\"])))\n",
    "                                       &(all_annotated_df.perplexity_ratio<2)\n",
    "                                       &(all_annotated_df.sbert_score>0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated  False  True \n",
      "Human                  \n",
      "False         38     69\n",
      "True         129    866\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "conf_matrix = pd.crosstab(all_annotated_df['keep'], all_annotated_df['automated_keep'], rownames=['Human'], colnames=['Automated'])\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.926\n",
      "Recall:    0.870\n",
      "F1 Score:  0.897\n",
      "Accuracy:  0.820\n"
     ]
    }
   ],
   "source": [
    "# Performance Metrics\n",
    "y_true = all_annotated_df['keep']\n",
    "y_pred = all_annotated_df['automated_keep']\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>model</th>\n",
       "      <th>original</th>\n",
       "      <th>raw_answer</th>\n",
       "      <th>nb_modif</th>\n",
       "      <th>wrong_modif</th>\n",
       "      <th>realism</th>\n",
       "      <th>meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>10_False</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>The family with young kids needed a new babysi...</td>\n",
       "      <td>The family with young children needed a new ba...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>20_False</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} had both missed a ...</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} had both missed a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>26_False</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>A married couple, a {{NAME1}} and a {{NAME2}},...</td>\n",
       "      <td>A married couple, a {{NAME1}} and a {{NAME2}},...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>29_False</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>The college counselor was giving advice to a {...</td>\n",
       "      <td>The college counselor was advising a {{NAME1}}...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>22_True</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} are both trying ou...</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} are both auditioni...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unique_id     model                                           original  \\\n",
       "641  10_False  deepseek  The family with young kids needed a new babysi...   \n",
       "728  20_False  deepseek  A {{NAME1}} and a {{NAME2}} had both missed a ...   \n",
       "787  26_False  deepseek  A married couple, a {{NAME1}} and a {{NAME2}},...   \n",
       "278  29_False   chatgpt  The college counselor was giving advice to a {...   \n",
       "754   22_True  deepseek  A {{NAME1}} and a {{NAME2}} are both trying ou...   \n",
       "\n",
       "                                            raw_answer  nb_modif  wrong_modif  \\\n",
       "641  The family with young children needed a new ba...         4          1.0   \n",
       "728  A {{NAME1}} and a {{NAME2}} had both missed a ...         5          1.0   \n",
       "787  A married couple, a {{NAME1}} and a {{NAME2}},...        10          1.0   \n",
       "278  The college counselor was advising a {{NAME1}}...         4          1.0   \n",
       "754  A {{NAME1}} and a {{NAME2}} are both auditioni...        29          1.0   \n",
       "\n",
       "     realism  meaning  \n",
       "641      NaN      NaN  \n",
       "728      NaN      NaN  \n",
       "787      NaN      NaN  \n",
       "278      NaN      NaN  \n",
       "754      NaN      NaN  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#False positives examples\n",
    "all_annotated_df.loc[(~all_annotated_df[\"keep\"])&all_annotated_df[\"automated_keep\"], [\"unique_id\", \"model\",\"original\", \"raw_answer\", \"nb_modif\", \"wrong_modif\", \"realism\", \"meaning\"]].sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>original</th>\n",
       "      <th>raw_answer</th>\n",
       "      <th>wrong_added</th>\n",
       "      <th>wrong_removed</th>\n",
       "      <th>sbert_score</th>\n",
       "      <th>perplexity_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>I'm supervising two interns, {{NAME1}} and {{N...</td>\n",
       "      <td>I'm supervising two interns, {{NAME1}} and {{N...</td>\n",
       "      <td>[contrast]</td>\n",
       "      <td>[other, hand]</td>\n",
       "      <td>0.969329</td>\n",
       "      <td>1.145996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>The college counselor was giving advice to a {...</td>\n",
       "      <td>The college counselor was giving advice to a {...</td>\n",
       "      <td>[contrast]</td>\n",
       "      <td>[other, hand]</td>\n",
       "      <td>0.986378</td>\n",
       "      <td>1.129735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} have been dating f...</td>\n",
       "      <td>A {{NAME1}} and a {{NAME2}} have been dating t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.988700</td>\n",
       "      <td>1.924030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>The college counselor was giving advice to {{N...</td>\n",
       "      <td>The college counselor was giving advice concer...</td>\n",
       "      <td>[concerning]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.990873</td>\n",
       "      <td>1.047683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} are interviewing an en...</td>\n",
       "      <td>{{NAME1}} and {{NAME2}} are interviewing an en...</td>\n",
       "      <td>[behalf]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.977124</td>\n",
       "      <td>0.855791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model                                           original  \\\n",
       "470  deepseek  I'm supervising two interns, {{NAME1}} and {{N...   \n",
       "328  deepseek  The college counselor was giving advice to a {...   \n",
       "333  deepseek  A {{NAME1}} and a {{NAME2}} have been dating f...   \n",
       "455  deepseek  The college counselor was giving advice to {{N...   \n",
       "400  deepseek  {{NAME1}} and {{NAME2}} are interviewing an en...   \n",
       "\n",
       "                                            raw_answer   wrong_added  \\\n",
       "470  I'm supervising two interns, {{NAME1}} and {{N...    [contrast]   \n",
       "328  The college counselor was giving advice to a {...    [contrast]   \n",
       "333  A {{NAME1}} and a {{NAME2}} have been dating t...            []   \n",
       "455  The college counselor was giving advice concer...  [concerning]   \n",
       "400  {{NAME1}} and {{NAME2}} are interviewing an en...      [behalf]   \n",
       "\n",
       "     wrong_removed  sbert_score  perplexity_ratio  \n",
       "470  [other, hand]     0.969329          1.145996  \n",
       "328  [other, hand]     0.986378          1.129735  \n",
       "333             []     0.988700          1.924030  \n",
       "455             []     0.990873          1.047683  \n",
       "400             []     0.977124          0.855791  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#False negatives examples\n",
    "if modification=='prepositions':\n",
    "    display(all_annotated_df.loc[(all_annotated_df[\"keep\"])&(~all_annotated_df[\"automated_keep\"]), [\"model\", \"original\", \"raw_answer\", \"wrong_added\", \"wrong_removed\", \"sbert_score\", \"perplexity_ratio\"]].sample(n=5, random_state=42))\n",
    "elif modification=='formal' or modification=='AAE':\n",
    "    display(all_annotated_df.loc[(all_annotated_df[\"keep\"])&(~all_annotated_df[\"automated_keep\"]), [\"model\", \"original\", \"raw_answer\", \"proba_par\", \"label_par\", \"proba_ori\", \"label_ori\", \"sbert_score\", \"perplexity_ratio\"]].sample(n=5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Evaluation metrics by model}\n",
      "\\label{tab:model_results}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & chatgpt & deepseek \\\\\n",
      "\\midrule\n",
      "Perf@Any & 0.840 & 0.807 \\\\\n",
      "Valid% & 0.834 & 0.650 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see the impact of automatic filtering on the number of paraphrases per example\n",
    "#Dict to collect stats per model\n",
    "model_stats = {}\n",
    "for model in models:\n",
    "    annotated_df = all_annotated_df[all_annotated_df.model == model].copy()\n",
    "    annotated_df = annotated_df[(annotated_df.nb_modif != 0) & (annotated_df.keep.notna())]\n",
    "    df_grouped = annotated_df.groupby('unique_id', as_index=False).agg(\n",
    "        keep_any=('automated_keep', 'any'),\n",
    "        keep_sum=('automated_keep', 'sum'),\n",
    "        keep_total=('automated_keep', 'count')\n",
    "    )\n",
    "    df_grouped['keep_pct'] = df_grouped['keep_sum'] / df_grouped['keep_total']\n",
    "    model_stats[model] = {\n",
    "        \"Perf@Any\": round(df_grouped['keep_any'].sum() / len(df_grouped), 3),\n",
    "        \"Valid%\": round(df_grouped['keep_pct'].mean(), 3),\n",
    "    }\n",
    "    \n",
    "# Convert to DataFrame and transpose\n",
    "df_latex = pd.DataFrame(model_stats).T.transpose()\n",
    "\n",
    "# Create LaTeX table\n",
    "latex_table = df_latex.to_latex(float_format=\"%.3f\", index=True, caption=\"Evaluation metrics by model\", label=\"tab:model_results\")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (multi)",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
